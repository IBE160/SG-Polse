Random Forest
What is a Random Forest?
A Random Forest is like a team of many decision trees working together.
Each tree makes its own prediction, and the forest takes a majority vote (for classification)
or average (for regression).
Think of it as â€œwisdom of the crowdâ€ â€” instead of trusting one decision tree, we trust many
trees and let them decide together.
How It Works (Step-by-Step)
1. 	Create many decision trees â€” each tree gets a slightly different random sample of
the data (this is called bagging).
2. 	Each tree learns patterns independently (like â€œHas stripes?â€, â€œHas mane?â€, etc.).
3. 	All trees vote on the final answer.
4. 	The majority vote wins â€” this gives better accuracy and avoids overfitting.
ğŸ¦ vs ğŸ¯ Example
Letâ€™s say we again want to predict whether an animal is a lion or a tiger, based on features
like:
Has Stripes Has Mane Weight (kg) Animal
0 	1 	190 	Lion
1 	0 	220 	Tiger
0 	1 	180 	Lion
1 	0 	200 	Tiger
â€¢ 	Tree 1 might decide: â€œHas stripes?â€ â†’ Tiger
â€¢ 	Tree 2 might decide: â€œHas mane?â€ â†’ Lion
â€¢ 	Tree 3 might use both â€œWeightâ€ and â€œHas stripesâ€
When we show a new animal:
â€¢ 	Tree 1 â†’ Tiger
â€¢ 	Tree 2 â†’ Tiger
â€¢ 	Tree 3 â†’ Lion
Majority vote = Tiger

-- 1 of 2 --

The Random Forest predicts: Tiger
Why Use Random Forest?
Feature 	Decision Tree 	Random Forest
Simplicity Easy to understand 	A bit complex
Stability 	Can overfit easily 	More stable and accurate
Accuracy 	Depends on one tree 	Combines many trees for better performance
Analogy 	One personâ€™s opinion Group decision (majority wins)
Summary
â€¢ 	Random Forest = many trees + random sampling + voting
â€¢ 	Reduces overfitting and improves accuracy
â€¢ 	Works well for both classification (like lion vs tiger) and regression problems

-- 2 of 2 --

